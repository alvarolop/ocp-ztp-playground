= OpenShift ZTP Playground
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2024-10
// Metadata
:description: This repository is my playground to keep all my work about installing and configuring a fleet of clusters using ZTP.
:keywords: openshift, ztp, installation, baremetal, red hat
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 3
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]

This repository is my playground to keep all my work about installing and configuring a fleet of clusters using ZTP.


// Create the Table of contents here
toc::[]

== Introduction

OpenShift Zero Touch Provisioning (ZTP) automates deploying and managing OpenShift clusters with minimal manual effort. It uses tools like Red Hat ACM and OpenShift GitOps for hardware discovery, installation, and configuration, enabling consistent, scalable deployments while reducing overhead and errors.

ZTP offers a comprehensive solution for managing clusters throughout their entire lifecycle—from installation to configuration and upgrades:

* *Baremetal cluster provisioning* via a straightforward site-config.yaml file, making the initial setup seamless.

* *Day 2 configuration* with PolicyGenTemplates, facilitating consistent post-deployment management and updates.

* *Software lifecycle management for multiple clusters* with the Topology-Aware Lifecycle Manager (TALM), enabling efficient, topology-aware upgrades and maintenance.




== ZTP Hub Installation

To successfully install Zero-Touch Provisioning (ZTP) for managing edge clusters in OpenShift, there are three essential components that need to be installed:

* *GitOps*: Uses Git workflows to automate and maintain cluster configurations, ensuring clusters match the desired state defined in Git.

* *Advanced Cluster Management (ACM)*: ACM acts as the central point for managing multiple clusters, providing visibility, governance, and control over edge deployments.

* *Topology-Aware Lifecycle Manager (TALM)*: TALM is crucial for managing the lifecycle of clusters, allowing for topology-aware updates and better coordination across the edge environment.


We are going to use GitOps to also install ACM and TALM. For that reason, please, access this repository https://github.com/alvarolop/ocp-gitops-playground[alvarolop/ocp-gitops-playground] and install GitOps using that automation. After that, create the following application:


[source, bash]
----
oc apply -f application-ztp-infra.yaml
----

Then, to allow ArgoCD to render the templates provided by ZTP, you can use the following patch:

[source, bash]
----
oc patch argocd argocd -n gitops --type=merge \
--patch-file ztp-infra/argocd-openshift-gitops-patch.json
----

Also, you will like to disable the `cluster-proxy-addon` addon from ACM Hub to ensure direct communication between ACM pods in OpenShift and ZTP subnet machines:

[source, bash]
----
oc patch multiclusterengines multiclusterengine --type=merge \
--patch-file ztp-infra/disable-cluster-proxy-addon.json
----


=== Enabling ACM Observability

After installing all the mandatory components for ZTP, it is also possible to enable ACM Observability component to gather observability events from the managed clusters. This component has several prerequisites that make it difficult [.line-through]#and nasty#, so I have created a post-install script that you can run right after the ZTP installation.

[source, bash]
----
./rhacm-obs/install.sh
----



== Sites Baremetal node installation


=== Step 0: AWS credentials

First, you will need to create an `aws-env-vars` file and fill the following variables:

[source, bash]
----
# CHECK VARIABLE VALUES ARE BETWEEN DOUBLE QUOTES!
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export AWS_DEFAULT_REGION=""
----

Then, you just need to source it to use the variables in the Ansible Playbook:

[source, bash]
----
source ./aws-env-vars
----

=== Step 1: Create an SSH Key pair


[source, bash]
----
ansible-playbook -vv ansible/aws_keypair_create.yml
----


=== Step 2: Deploy an instance on AWS

Now, create a baremetal instance on AWS using the following Ansible Playbook:

[source, bash]
----
ansible-playbook -vv ansible/aws_ec2_create_vm.yml
----

You should be able to connect to your instance using a command like the following:

[source, bash]
----
ssh -i ~/.ssh/aws-ztp-key.pem ec2-user@$PublicDnsName
----

If you forgot the DNS name, you can use the following command:

[source, bash]
----
aws ec2 describe-instances | jq -r '.Reservations[].Instances[] | "[\(.State.Name)] \(.Tags[] | select(.Key == "Name").Value) \(.PublicDnsName)"' 
----

=== Step 3: Install all the related configuration on the RHEL 9 node

Now, it is time to install all the important packages inside the RHEL node. 

First, copy the `$PublicDnsName` to the `inventory` file. The content should be something like:

[source, ini]
----
[ztp]
$PublicDnsName ansible_user=ec2-user ansible_ssh_private_key_file=~/.ssh/aws-ztp-key.pem
----

Then execute the following Ansible playbook:

[source, bash]
----
ansible-playbook -vv ansible/aws_ec2_configure_vm.yml
----



== Testing deployment


[source, bash]
----
curl -k https://localhost:9000/redfish/v1/Systems/local
----


== Useful Links

* Documentation - https://docs.openshift.com/container-platform/4.16/edge_computing/ztp-deploying-far-edge-clusters-at-scale.html[Challenges of the network far edge]: This is the main explanation of GitOps ZTP provisioning.
* Documentation - https://docs.openshift.com/container-platform/4.16/scalability_and_performance/telco_ref_design_specs/ran/telco-ran-ref-design-spec.html[Telco RAN DU 4.16 reference design overview] to configure OCP on commodity hardware to host telco RAN DU workloads.
* Documentation - https://docs.openshift.com/container-platform/4.16/edge_computing/policygentemplate_for_ztp/ztp-configuring-managed-clusters-policies.html[Configuring managed cluster policies by using PolicyGenTemplate resources] - Deprecated.
* Documentation - https://docs.openshift.com/container-platform/4.16/edge_computing/policygenerator_for_ztp/ztp-configuring-managed-clusters-policygenerator.html[Configuring managed cluster policies by using PolicyGenerator resources] - Tech Preview.

* Workshop - https://labs.sysdeseng.com/5g-ran-deployments-on-ocp-lab/4.16/index.html[Red Hat Lab 5G RAN Deployments on OpenShift].



:!sectnums:

== Annex A: Deploy Development Grafana instance

You can design your Grafana dashboard by creating a grafana-dev instance. Follow these steps

1. Clone the upstream repo:
+
[source, bash]
----
git clone https://github.com/stolostron/multicluster-observability-operator.git
----
+
2. Change to the tools folder:
+
[source, bash]
----
cd multicluster-observability-operator/tools
----
+
3. Deploy Grafana instance:
+
[source, bash]
----
./setup-grafana-dev.sh --deploy
----
+
4. Make yourself Grafana admin:
+
[source, bash]
----
./switch-to-grafana-admin.sh $(oc whoami)
----
+
5. Now, access the Grafana console, create a dashboard named `Alvaro - Custom Overview` and edit is as you wish. Then, you can export it with:
+
[source, bash]
----
./generate-dashboard-configmap-yaml.sh "Alvaro - Custom Overview"
----
+
6. Finally, you can promote the dashboard to the production Grafana creating the ConfigMap:
+
[source, bash]
----
oc apply -n open-cluster-management-observability -f alvaro---custom-overview.yaml
----
+
7. When you are done, you can delete the dev instance with the following command:
+
[source, bash]
----
./setup-grafana-dev.sh --clean
----
